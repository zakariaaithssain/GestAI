{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91f32058",
   "metadata": {},
   "source": [
    "# POC NOTEBOOK #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee59acda",
   "metadata": {},
   "source": [
    "### Note: To launch the vLLM server, only run the last cell, run the cells before only once to configure the env."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff4afe5",
   "metadata": {},
   "source": [
    "chosen model: Qwen3-0.6B, very lightweight and pretty enough for PoC: https://huggingface.co/Qwen/Qwen3-0.6B-Base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bf6243",
   "metadata": {},
   "source": [
    "virtual env config: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4118deba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mThe `--system` flag has no effect, a system Python interpreter is always used in `uv venv`\u001b[0m\n",
      "Using CPython 3.12.12 interpreter at: \u001b[36m/usr/bin/python3\u001b[39m\n",
      "Creating virtual environment with seed packages at: \u001b[36m.poc\u001b[39m\n",
      "\u001b[33m?\u001b[0m \u001b[1mA virtual environment already exists at `.poc`. Do you want to replace it?\u001b[0m \u001b[38;5;8m[y/n]\u001b[0m \u001b[38;5;8mâ€º\u001b[0m \u001b[36myes\u001b[0m\n",
      "\n",
      "\u001b[36m\u001b[1mhint\u001b[0m\u001b[1m:\u001b[0m Use the `\u001b[32m--clear\u001b[39m` flag or set `\u001b[32mUV_VENV_CLEAR=1\u001b[39m` to skip this prompt\u001b[?25l^C\n",
      "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m4 packages\u001b[0m \u001b[2min 103ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv venv .poc --python 3.12 --seed\n",
    "!source .poc/bin/activate \n",
    "!uv pip install vllm openai gspread oauth2client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8bdd21",
   "metadata": {},
   "source": [
    "Qwen-VL utility (for offline inference): https://docs.vllm.ai/projects/recipes/en/latest/Qwen/Qwen3-VL.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2da61bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m9 packages\u001b[0m \u001b[2min 113ms\u001b[0m\u001b[0m                                         \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m2 packages\u001b[0m \u001b[2min 505ms\u001b[0m\u001b[0m                                             \n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m2 packages\u001b[0m \u001b[2min 15ms\u001b[0m\u001b[0m                                \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mav\u001b[0m\u001b[2m==16.0.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mqwen-vl-utils\u001b[0m\u001b[2m==0.0.14\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install qwen-vl-utils==0.0.14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889bff68",
   "metadata": {},
   "source": [
    "install the model and run the vllm server: (this cell will keep the vLLM server running)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dd5752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-27 21:57:26.735501: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764280646.755450   12365 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764280646.761529   12365 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1764280646.776774   12365 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764280646.776800   12365 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764280646.776805   12365 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764280646.776809   12365 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-11-27 21:57:26.781402: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "INFO 11-27 21:57:35 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO 11-27 21:57:35 [api_server.py:1977] vLLM API server version 0.11.2\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO 11-27 21:57:35 [utils.py:253] non-default args: {'host': '0.0.0.0', 'gpu_memory_utilization': 0.6, 'max_num_seqs': 16}\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO 11-27 21:57:35 [model.py:631] Resolved architecture: Qwen3ForCausalLM\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m WARNING 11-27 21:57:35 [model.py:1921] Your device 'Tesla T4' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m WARNING 11-27 21:57:35 [model.py:1971] Casting torch.bfloat16 to torch.float16.\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO 11-27 21:57:35 [model.py:1745] Using max model len 40960\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO 11-27 21:57:37 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "2025-11-27 21:57:45.323785: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764280665.344372   12457 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764280665.350807   12457 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1764280665.366357   12457 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764280665.366383   12457 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764280665.366388   12457 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764280665.366391   12457 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12457)\u001b[0;0m INFO 11-27 21:57:52 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-0.6B', speculative_config=None, tokenizer='Qwen/Qwen3-0.6B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-0.6B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 32, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12457)\u001b[0;0m ERROR 11-27 21:57:53 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12457)\u001b[0;0m INFO 11-27 21:57:53 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.28.0.12:41645 backend=nccl\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12457)\u001b[0;0m INFO 11-27 21:57:53 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12457)\u001b[0;0m INFO 11-27 21:57:54 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-0.6B...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12457)\u001b[0;0m INFO 11-27 21:57:55 [cuda.py:418] Valid backends: ['FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12457)\u001b[0;0m INFO 11-27 21:57:55 [cuda.py:427] Using FLASHINFER backend.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12457)\u001b[0;0m INFO 11-27 21:57:56 [weight_utils.py:481] No model.safetensors.index.json found in remote.\n",
      "Loading safetensors checkpoint shards: 100% 1/1 [00:01<00:00,  1.46s/it]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12457)\u001b[0;0m INFO 11-27 21:57:57 [default_loader.py:314] Loading weights took 1.49 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12457)\u001b[0;0m INFO 11-27 21:57:58 [gpu_model_runner.py:3338] Model loading took 1.1201 GiB memory and 2.899380 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12457)\u001b[0;0m INFO 11-27 21:58:06 [backends.py:631] Using cache directory: /root/.cache/vllm/torch_compile_cache/703c77181e/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12457)\u001b[0;0m INFO 11-27 21:58:06 [backends.py:647] Dynamo bytecode transform time: 7.75 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12457)\u001b[0;0m INFO 11-27 21:58:13 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.438 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12457)\u001b[0;0m INFO 11-27 21:58:15 [monitor.py:34] torch.compile takes 14.19 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12457)\u001b[0;0m INFO 11-27 21:58:16 [gpu_worker.py:359] Available KV cache memory: 7.57 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12457)\u001b[0;0m INFO 11-27 21:58:17 [kv_cache_utils.py:1229] GPU KV cache size: 70,848 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12457)\u001b[0;0m INFO 11-27 21:58:17 [kv_cache_utils.py:1234] Maximum concurrency for 40,960 tokens per request: 1.73x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12457)\u001b[0;0m INFO 11-27 21:58:17 [kernel_warmup.py:65] Warming up FlashInfer attention.\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100% 7/7 [00:00<00:00, 19.37it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100% 5/5 [00:00<00:00, 17.91it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12457)\u001b[0;0m INFO 11-27 21:58:18 [gpu_model_runner.py:4244] Graph capturing finished in 2 secs, took 0.14 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12457)\u001b[0;0m INFO 11-27 21:58:18 [core.py:250] init engine (profile, create kv cache, warmup model) took 20.35 seconds\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m [INFO] model_hosting_container_standards - decorators.py:76: [PING] Framework handler registered: ping\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO:model_hosting_container_standards:[PING] Framework handler registered: ping\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m [INFO] model_hosting_container_standards.common.transforms.base_factory - base_factory.py:90: [INJECT_ADAPTER_ID] Transform decorator applied to: invocations\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO:model_hosting_container_standards.common.transforms.base_factory:[INJECT_ADAPTER_ID] Transform decorator applied to: invocations\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m [INFO] model_hosting_container_standards.common.transforms.base_factory - base_factory.py:115: [INJECT_ADAPTER_ID] Registered transform handler for invocations\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO:model_hosting_container_standards.common.transforms.base_factory:[INJECT_ADAPTER_ID] Registered transform handler for invocations\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m [INFO] model_hosting_container_standards.common.transforms.base_factory - base_factory.py:90: [STATEFUL_SESSION_MANAGER] Transform decorator applied to: decorated_func\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO:model_hosting_container_standards.common.transforms.base_factory:[STATEFUL_SESSION_MANAGER] Transform decorator applied to: decorated_func\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m [INFO] model_hosting_container_standards.common.transforms.base_factory - base_factory.py:115: [STATEFUL_SESSION_MANAGER] Registered transform handler for decorated_func\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO:model_hosting_container_standards.common.transforms.base_factory:[STATEFUL_SESSION_MANAGER] Registered transform handler for decorated_func\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m [INFO] model_hosting_container_standards - decorators.py:76: [INVOKE] Framework handler registered: decorated_func\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO:model_hosting_container_standards:[INVOKE] Framework handler registered: decorated_func\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m [INFO] model_hosting_container_standards - __init__.py:127: Starting SageMaker bootstrap process\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO:model_hosting_container_standards:Starting SageMaker bootstrap process\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m [INFO] model_hosting_container_standards - registry.py:109: [REGISTRY] Middleware resolution and registration complete\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO:model_hosting_container_standards:[REGISTRY] Middleware resolution and registration complete\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m [INFO] model_hosting_container_standards - core.py:100: [MIDDLEWARE_LOADER] Middleware stack rebuilt successfully\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO:model_hosting_container_standards:[MIDDLEWARE_LOADER] Middleware stack rebuilt successfully\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m [INFO] model_hosting_container_standards - core.py:102: [MIDDLEWARE_LOADER] Processed 3 middlewares\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO:model_hosting_container_standards:[MIDDLEWARE_LOADER] Processed 3 middlewares\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m [WARNING] model_hosting_container_standards.common.custom_code_ref_resolver.function_loader - function_loader.py:73: Failed to load function from spec 'model:custom_sagemaker_invocation_handler': HandlerFileNotFoundError: File '/opt/ml/model/model.py' not found in search paths: ['/opt/ml/model/']\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m WARNING:model_hosting_container_standards.common.custom_code_ref_resolver.function_loader:Failed to load function from spec 'model:custom_sagemaker_invocation_handler': HandlerFileNotFoundError: File '/opt/ml/model/model.py' not found in search paths: ['/opt/ml/model/']\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m [WARNING] model_hosting_container_standards.common.custom_code_ref_resolver.function_loader - function_loader.py:73: Failed to load function from spec 'model:custom_sagemaker_ping_handler': HandlerFileNotFoundError: File '/opt/ml/model/model.py' not found in search paths: ['/opt/ml/model/']\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m WARNING:model_hosting_container_standards.common.custom_code_ref_resolver.function_loader:Failed to load function from spec 'model:custom_sagemaker_ping_handler': HandlerFileNotFoundError: File '/opt/ml/model/model.py' not found in search paths: ['/opt/ml/model/']\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m [INFO] model_hosting_container_standards.sagemaker.sagemaker_router - sagemaker_router.py:93: Creating SageMaker router with unified route resolver\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO:model_hosting_container_standards.sagemaker.sagemaker_router:Creating SageMaker router with unified route resolver\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m [INFO] model_hosting_container_standards.common.fastapi.routing - routing.py:172: Creating router with prefix='', tags=['sagemaker']\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO:model_hosting_container_standards.common.fastapi.routing:Creating router with prefix='', tags=['sagemaker']\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m [INFO] model_hosting_container_standards.common.fastapi.routing - routing.py:110: Mounting 2 handlers to router\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO:model_hosting_container_standards.common.fastapi.routing:Mounting 2 handlers to router\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m [INFO] model_hosting_container_standards.common.fastapi.routing - routing.py:184: Router created with 0 routes\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO:model_hosting_container_standards.common.fastapi.routing:Router created with 0 routes\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m [INFO] model_hosting_container_standards.sagemaker.sagemaker_router - sagemaker_router.py:101: SageMaker router created successfully with 0 routes\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO:model_hosting_container_standards.sagemaker.sagemaker_router:SageMaker router created successfully with 0 routes\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m [INFO] model_hosting_container_standards.common.fastapi.routing - routing.py:287: Including router with conflict detection\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO:model_hosting_container_standards.common.fastapi.routing:Including router with conflict detection\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m [INFO] model_hosting_container_standards.common.fastapi.routing - routing.py:305: Successfully included router with 0 routes\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO:model_hosting_container_standards.common.fastapi.routing:Successfully included router with 0 routes\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m [INFO] model_hosting_container_standards - __init__.py:139: SageMaker bootstrap completed successfully\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO:model_hosting_container_standards:SageMaker bootstrap completed successfully\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO 11-27 21:58:21 [api_server.py:1725] Supported tasks: ['generate']\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m WARNING 11-27 21:58:21 [model.py:1568] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO 11-27 21:58:21 [serving_responses.py:154] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO 11-27 21:58:21 [serving_chat.py:131] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO 11-27 21:58:21 [serving_completion.py:73] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO 11-27 21:58:21 [serving_chat.py:131] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO 11-27 21:58:21 [api_server.py:2052] Starting vLLM API server 0 on http://0.0.0.0:8000\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO 11-27 21:58:21 [launcher.py:38] Available routes are:\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO 11-27 21:58:21 [launcher.py:46] Route: /openapi.json, Methods: HEAD, GET\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO 11-27 21:58:21 [launcher.py:46] Route: /docs, Methods: HEAD, GET\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO 11-27 21:58:21 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO 11-27 21:58:21 [launcher.py:46] Route: /redoc, Methods: HEAD, GET\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO 11-27 21:58:21 [launcher.py:46] Route: /health, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO 11-27 21:58:21 [launcher.py:46] Route: /load, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO 11-27 21:58:21 [launcher.py:46] Route: /tokenize, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO 11-27 21:58:21 [launcher.py:46] Route: /detokenize, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO 11-27 21:58:21 [launcher.py:46] Route: /v1/models, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO 11-27 21:58:21 [launcher.py:46] Route: /version, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO 11-27 21:58:21 [launcher.py:46] Route: /v1/responses, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO 11-27 21:58:21 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO 11-27 21:58:21 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO 11-27 21:58:21 [launcher.py:46] Route: /v1/messages, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO 11-27 21:58:21 [launcher.py:46] Route: /v1/chat/completions, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO 11-27 21:58:21 [launcher.py:46] Route: /v1/completions, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO 11-27 21:58:21 [launcher.py:46] Route: /v1/embeddings, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO 11-27 21:58:21 [launcher.py:46] Route: /pooling, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO 11-27 21:58:21 [launcher.py:46] Route: /classify, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO 11-27 21:58:21 [launcher.py:46] Route: /score, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO 11-27 21:58:21 [launcher.py:46] Route: /v1/score, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO 11-27 21:58:21 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO 11-27 21:58:21 [launcher.py:46] Route: /v1/audio/translations, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO 11-27 21:58:21 [launcher.py:46] Route: /rerank, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO 11-27 21:58:21 [launcher.py:46] Route: /v1/rerank, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO 11-27 21:58:21 [launcher.py:46] Route: /v2/rerank, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO 11-27 21:58:21 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO 11-27 21:58:21 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO 11-27 21:58:21 [launcher.py:46] Route: /inference/v1/generate, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO 11-27 21:58:21 [launcher.py:46] Route: /ping, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO 11-27 21:58:21 [launcher.py:46] Route: /ping, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO 11-27 21:58:21 [launcher.py:46] Route: /invocations, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m INFO 11-27 21:58:21 [launcher.py:46] Route: /metrics, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m \u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m12365\u001b[0m]\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m \u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m \u001b[32mINFO\u001b[0m:     Application startup complete.\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m \u001b[32mINFO\u001b[0m:     127.0.0.1:53674 - \"\u001b[1mGET /v1/ HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m \u001b[32mINFO\u001b[0m:     127.0.0.1:53678 - \"\u001b[1mGET /v1/models HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m \u001b[32mINFO\u001b[0m:     127.0.0.1:48820 - \"\u001b[1mGET /v1/models HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m \u001b[32mINFO\u001b[0m:     127.0.0.1:43644 - \"\u001b[1mGET /v1/models HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m \u001b[32mINFO\u001b[0m:     127.0.0.1:57886 - \"\u001b[1mGET /v1/models HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m \u001b[32mINFO\u001b[0m:     127.0.0.1:58542 - \"\u001b[1mGET /v1/models HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m \u001b[32mINFO\u001b[0m:     127.0.0.1:60232 - \"\u001b[1mGET /v1/models HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[1;36m(APIServer pid=12365)\u001b[0;0m \u001b[32mINFO\u001b[0m:     127.0.0.1:51776 - \"\u001b[1mGET /v1/models HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m vllm.entrypoints.openai.api_server \\\n",
    "    --model Qwen/Qwen3-0.6B \\\n",
    "    --port 8000 \\\n",
    "    --host 0.0.0.0 \\\n",
    "    --gpu-memory-utilization 0.6 \\\n",
    "    --max-num-seqs 16\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
